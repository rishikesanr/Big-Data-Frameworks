{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af6c7d2-3449-44d1-807e-76509e32fe91",
   "metadata": {},
   "source": [
    "# Hyperparameter-Tuning with GridSearch using a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c0dd7-46fc-4727-b7f6-74319bb5d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "\n",
    "airbnb_df = (spark\n",
    "            .read\n",
    "            .format(\"delta\")\n",
    "            .load(file_path)\n",
    "            .withColumn(\"priceClass\", (col(\"price\") >= 150).cast(\"int\"))\n",
    "            .drop(\"price\")\n",
    "           )\n",
    "\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"priceClass\"))]\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe1371-31d2-4c87-b28a-f107144fc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"priceClass\",\n",
    "                            maxBins=40,\n",
    "                            seed=38)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb0d3c-6d36-41c5-916c-7cb0a20b9210",
   "metadata": {},
   "source": [
    "## Grid Search \n",
    "\n",
    "Let's define a grid of hyperparameters to test:\n",
    "  - maxDepth: max depth of the decision tree (Use the values **`2, 5, 10`**)\n",
    "  - numTrees: number of decision trees (Use the values **`10, 20, 100`**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b7e76-7347-477f-bc97-316b8005707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(rf.maxDepth,[2,5,10]).addGrid(rf.numTrees,[10,20,100]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68206a3f-1ede-4982-864d-edc18fdcc6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator \n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"priceClass\",metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda5bf2-c593-40b9-a0db-612d19907027",
   "metadata": {},
   "source": [
    "## Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5a6fb-c684-49ec-8e2e-4a6aa229db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=rf, evaluator=evaluator,estimatorParamMaps=grid,numFolds=3,seed=38)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f1cd1-f054-42f8-846c-1ae7cb79dd53",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c80bd-0165-430e-8fad-674198464879",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [string_indexer, vec_assembler, cv]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f5731-dd27-436c-8c71-73b6e16492a7",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ca36b-314f-49d4-9828-696e8cdb5350",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = pipeline_model.stages[-1]\n",
    "rf_model = cv_model.bestModel\n",
    "\n",
    "# list(zip(cv_model.getEstimatorParamMaps(), cv_model.avgMetrics))\n",
    "\n",
    "print(rf_model.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad2f0d-3c05-4fdf-b7e0-bf96f546ea08",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d2695-202d-4b7f-a909-6bae36582a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame(list(zip(vec_assembler.getInputCols(), rf_model.featureImportances)), columns=[\"feature\", \"importance\"])\n",
    "top_features = pandas_df.sort_values([\"importance\"], ascending=False)\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baebe97-7512-4164-ac99-c80508323bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model now\n",
    "\n",
    "\n",
    "pred_df = pipeline_model.transform(test_df)\n",
    "area_under_roc = evaluator.evaluate(pred_df)\n",
    "print(f\"Area under ROC is {area_under_roc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:epl_analytics] *",
   "language": "python",
   "name": "conda-env-epl_analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
