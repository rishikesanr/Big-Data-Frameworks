{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c8c6e1-0d10-4768-94de-e693f9242a5f",
   "metadata": {},
   "source": [
    "# Distributed Hyperparameter Tuning with Hyperopt\n",
    "\n",
    "Note. Only for single node models, not for Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd854a6-0d92-4738-8c28-f4f496727a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(f\"{DA.paths.datasets}/airbnb/sf-listings/airbnb-cleaned.csv\".replace(\"dbfs:/\", \"/dbfs/\")).drop([\"zipcode\"], axis=1)\n",
    "\n",
    "file_path = 'dbfs:/mnt/dbacademy-datasets/scalable-machine-learning-with-apache-spark/v02/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/'\n",
    "print(file_path)\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "df = airbnb_df.toPandas()\n",
    "\n",
    "# split 80/20 train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1),\n",
    "                                                    df[[\"price\"]].values.ravel(),\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ee34f-09fc-468c-89ca-06d79639438b",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a87edf-e7f3-47eb-bcb5-102feed5560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from numpy import mean\n",
    "  \n",
    "def objective_function(params):\n",
    "    # set the hyperparameters that we want to tune:\n",
    "    max_depth = params[\"max_depth\"]\n",
    "    max_features = params[\"max_features\"]\n",
    "\n",
    "    regressor = RandomForestRegressor(max_depth=max_depth, max_features=max_features, random_state=42)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    r2 = mean(cross_val_score(regressor, X_train, y_train, cv=3))\n",
    "\n",
    "    # Note: since we aim to maximize r2, we need to return it as a negative value (\"loss\": -metric)\n",
    "    return -r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e11d498-1522-4e1b-9f88-05affce9305e",
   "metadata": {},
   "source": [
    "### Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a147daf-ad4a-450b-b027-9ecc87e759e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "max_features_choices =  [\"auto\", \"sqrt\", \"log2\"]\n",
    "search_space = {\n",
    "    \"max_depth\": hp.quniform(\"max_depth\",2,10,1),\n",
    "    \"max_features\": hp.choice(\"max_features\",max_features_choices)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04151ef8-c4cd-4270-8e2f-f6aea92e7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Models Parallely for different hyperparameter configuration\n",
    "\n",
    "from hyperopt import fmin, tpe, SparkTrials\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "# Number of models to evaluate\n",
    "num_evals = 8\n",
    "# Number of models to train concurrently\n",
    "spark_trials = SparkTrials(parallelism=1)\n",
    "# Automatically logs to MLflow\n",
    "best_hyperparam = fmin(fn=objective_function,\n",
    "                       space=search_space,\n",
    "                       max_evals=num_evals,\n",
    "                       trials=spark_trials,\n",
    "                       algo=tpe.suggest)\n",
    "\n",
    "# Re-train best model and log metrics on test dataset\n",
    "with mlflow.start_run(run_name=\"best_model\"):\n",
    "    # get optimal hyperparameter values\n",
    "    best_max_depth = best_hyperparam[\"max_depth\"]\n",
    "    best_max_features = max_features_choices[best_hyperparam[\"max_features\"]]\n",
    "    print(best_max_features)\n",
    "\n",
    "    # train model on entire training data\n",
    "    regressor = RandomForestRegressor(max_depth=best_max_depth, max_features=best_max_features, random_state=42)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate on holdout/test data\n",
    "    r2 = regressor.score(X_test, y_test)\n",
    "\n",
    "    # Log param and metric for the final model\n",
    "    mlflow.log_param(\"max_depth\", best_max_depth)\n",
    "    mlflow.log_param(\"max_features\", best_max_features)\n",
    "    mlflow.log_metric(\"loss\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:epl_analytics] *",
   "language": "python",
   "name": "conda-env-epl_analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
