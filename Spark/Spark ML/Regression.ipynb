{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50acc83-49c6-428b-a678-9608deb6d8d9",
   "metadata": {},
   "source": [
    "# Regression (Notebook for Databricks)\n",
    "\n",
    "We'll do the following steps to build our model, \n",
    "\n",
    "Steps:\n",
    "1. Use the features: **`bedrooms`**, **`bathrooms`**, **`bathrooms_na`**, **`minimum_nights`**, and **`number_of_reviews`** as input to your VectorAssembler.\n",
    "2. Build a Linear Regression Model\n",
    "3. Evaluate the **`RMSE`** and the **`R2`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef99d85-ef0d-4001-870d-5cd25676129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# from pyspark import SparkConf, SparkContext\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil import relativedelta\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_timestamp, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import collect_list, collect_set, concat, first, array_distinct, col, size, expr\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d714c09-0c48-452b-bec7-39d96a35f77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 18:49:53 WARN Utils: Your hostname, Rishikesans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.26 instead (on interface en0)\n",
      "24/05/01 18:49:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/01 18:49:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start the spark session (Although it is not required if notebook directly ran in Databricks)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Data Analysis in Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c085e-559f-41a6-a452-208675cce099",
   "metadata": {},
   "source": [
    "## Load Dataset and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fccde4-f0ee-49c7-8dfb-c2a0b6864394",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./cleaned_listings.csv\"\n",
    "# airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "\n",
    "#Read the cleaned csv file \n",
    "airbnb_df = spark.read.csv(file_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75927add-213d-43a2-af5d-d21b92d7045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 18:50:21 WARN Instrumentation: [a14b8fd5] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/05/01 18:50:22 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/01 18:50:22 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "#Vectorize the dependent variables \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"bedrooms\",\"bathrooms\",\"bathrooms_na\",\"minimum_nights\",\"number_of_reviews\"],outputCol=\"features\")\n",
    "\n",
    "vtrain_df = vec_assembler.transform(train_df)\n",
    "vtest_df = vec_assembler.transform(test_df)\n",
    "\n",
    "lr_model = LinearRegression(labelCol=\"price\").fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b34aab-37a1-48ba-a1a9-bcca6f6484d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 419.0121578376188\n",
      "R2 is 0.07456171275982248\n"
     ]
    }
   ],
   "source": [
    "pred_df = lr_model.transform(vtest_df)\n",
    "\n",
    "regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "rmse = regression_evaluator.evaluate(pred_df)\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b500f04-79a0-4909-a967-82cf09bf6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOS R-squared is low. Note this notebook is only for practice for Spark ML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2448ff98-c151-4fa8-bfa4-cbb6d961166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedrooms 114.17777449189113\n",
      "bathrooms -5.8636569331674835\n",
      "bathrooms_na -93.46199646696445\n",
      "minimum_nights 0.11479885115899408\n",
      "number_of_reviews -0.2841304691298576\n",
      "intercept: 89.84420157032639\n"
     ]
    }
   ],
   "source": [
    "for col, coef in zip(vec_assembler.getInputCols(), lr_model.coefficients):\n",
    "    print(col, coef)\n",
    "  \n",
    "print(f\"intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f228e-c207-4b10-b5f1-3e37d8507d38",
   "metadata": {},
   "source": [
    "## Additional notes on Spark Distributed Computing\n",
    "\n",
    "## Distributed Setting\n",
    "\n",
    "Although we can quickly solve for the parameters when the data is small, the closed form solution doesn't scale well to large datasets. \n",
    "\n",
    "Spark uses the following approach to solve a linear regression problem:\n",
    "\n",
    "* First, Spark tries to use matrix decomposition to solve the linear regression problem. \n",
    "* If it fails, Spark then uses <a href=\"https://spark.apache.org/docs/latest/ml-advanced.html#limited-memory-bfgs-l-bfgs\" target=\"_blank\">L-BFGS</a> to solve for the parameters. L-BFGS is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables. The <a href=\"https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\" target=\"_blank\">BFGS</a> method belongs to <a href=\"https://en.wikipedia.org/wiki/Quasi-Newton_method\" target=\"_blank\">quasi-Newton methods</a>, which are used to either find zeroes or local maxima and minima of functions iteratively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13001f80-a7d8-4a4a-9447-a1b677b6280f",
   "metadata": {},
   "source": [
    "## Improving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f2dbd3-1c2a-4eee-9260-73120299424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc214e-545e-4033-bae2-37fca303d227",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b833c13b-e5f7-4d7a-8b3f-ed7b0e687b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Variables \n",
    "\n",
    "#One Hot Encoding\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "ohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "ohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8175606-8f52-4b57-aa38-7a43b2c1be33",
   "metadata": {},
   "source": [
    "#### Vector Assembler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca0a694-4ca7-4a98-bd45-d290730c9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assembler_inputs = ohe_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb48ed-68bc-4d7d-add7-80dd42bf9c2f",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1fe490c-e944-4989-af77-bf0cc62d5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36244133-ef57-475a-924d-0101c0629137",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "307197f2-6d25-4690-926c-c02d6dd21ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 19:06:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/05/01 19:06:11 WARN Instrumentation: [f6d3f813] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/05/01 19:06:12 WARN Instrumentation: [f6d3f813] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = [string_indexer, ohe_encoder, vec_assembler, lr]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a12f0-e65e-4218-bdd4-9663cf93b3ee",
   "metadata": {},
   "source": [
    "#### Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49d1658b-541f-4709-8aea-99278e7d991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.write().overwrite().save('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85b973f-2712-40e0-b494-95765c11ad0e",
   "metadata": {},
   "source": [
    "#### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9faa80b9-4d6a-42d0-b4cc-219e3c212abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "saved_pipeline_model = PipelineModel.load('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d52b08-cf16-4d13-a591-f3edfb1c54f0",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfde2ad-a1c8-4795-8167-2b82e901bec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, price: double, prediction: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_df = saved_pipeline_model.transform(test_df)\n",
    "\n",
    "display(pred_df.select(\"features\", \"price\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "194a7419-bff6-4819-813b-9c8679f6fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e7f7e-22b5-49d0-ab96-1740ce149f54",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c22df66-e481-4d94-ac63-1b98184d91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "\n",
    "# rmse = regression_evaluator.evaluate(pred_df)\n",
    "# r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "# print(f\"RMSE is {rmse}\")\n",
    "# print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75501e25-d659-4fb9-bdc4-6ea64ebc4ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:epl_analytics] *",
   "language": "python",
   "name": "conda-env-epl_analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
