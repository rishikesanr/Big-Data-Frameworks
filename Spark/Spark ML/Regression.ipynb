{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50acc83-49c6-428b-a678-9608deb6d8d9",
   "metadata": {},
   "source": [
    "# Regression (Notebook for Databricks)\n",
    "\n",
    "We'll do the following steps to build our model, \n",
    "\n",
    "Steps:\n",
    "1. Use the features: **`bedrooms`**, **`bathrooms`**, **`bathrooms_na`**, **`minimum_nights`**, and **`number_of_reviews`** as input to your VectorAssembler.\n",
    "2. Build a Linear Regression Model\n",
    "3. Evaluate the **`RMSE`** and the **`R2`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef99d85-ef0d-4001-870d-5cd25676129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# from pyspark import SparkConf, SparkContext\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil import relativedelta\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_timestamp, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import collect_list, collect_set, concat, first, array_distinct, col, size, expr\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d714c09-0c48-452b-bec7-39d96a35f77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 17:53:00 WARN Utils: Your hostname, Rishikesans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.26 instead (on interface en0)\n",
      "24/05/01 17:53:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/01 17:53:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/01 17:53:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Start the spark session (Although it is not required if notebook directly ran in Databricks)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Data Analysis in Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c085e-559f-41a6-a452-208675cce099",
   "metadata": {},
   "source": [
    "## Load Dataset and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fccde4-f0ee-49c7-8dfb-c2a0b6864394",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./cleaned_listings.csv\"\n",
    "# airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "\n",
    "#Read the cleaned csv file \n",
    "airbnb_df = spark.read.csv(file_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c52b5-6626-4321-b15d-406b7127de1a",
   "metadata": {},
   "source": [
    "#Vectorize the dependent variables \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"bedrooms\",\"bathrooms\",\"bathrooms_na\",\"minimum_nights\",\"number_of_reviews\"],outputCol=\"features\")\n",
    "\n",
    "vtrain_df = vec_assembler.transform(train_df)\n",
    "vtest_df = vec_assembler.transform(test_df)\n",
    "\n",
    "lr_model = LinearRegression(labelCol=\"price\").fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b34aab-37a1-48ba-a1a9-bcca6f6484d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 419.0121578376188\n",
      "R2 is 0.07456171275982248\n"
     ]
    }
   ],
   "source": [
    "pred_df = lr_model.transform(vtest_df)\n",
    "\n",
    "regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "rmse = regression_evaluator.evaluate(pred_df)\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b500f04-79a0-4909-a967-82cf09bf6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOS R-squared is low. Note this notebook is only for practice for Spark ML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2448ff98-c151-4fa8-bfa4-cbb6d961166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedrooms 114.17777449189113\n",
      "bathrooms -5.8636569331674835\n",
      "bathrooms_na -93.46199646696445\n",
      "minimum_nights 0.11479885115899408\n",
      "number_of_reviews -0.2841304691298576\n",
      "intercept: 89.84420157032639\n"
     ]
    }
   ],
   "source": [
    "for col, coef in zip(vec_assembler.getInputCols(), lr_model.coefficients):\n",
    "    print(col, coef)\n",
    "  \n",
    "print(f\"intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f228e-c207-4b10-b5f1-3e37d8507d38",
   "metadata": {},
   "source": [
    "## Additional notes on Spark Distributed Computing\n",
    "\n",
    "## Distributed Setting\n",
    "\n",
    "Although we can quickly solve for the parameters when the data is small, the closed form solution doesn't scale well to large datasets. \n",
    "\n",
    "Spark uses the following approach to solve a linear regression problem:\n",
    "\n",
    "* First, Spark tries to use matrix decomposition to solve the linear regression problem. \n",
    "* If it fails, Spark then uses <a href=\"https://spark.apache.org/docs/latest/ml-advanced.html#limited-memory-bfgs-l-bfgs\" target=\"_blank\">L-BFGS</a> to solve for the parameters. L-BFGS is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables. The <a href=\"https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\" target=\"_blank\">BFGS</a> method belongs to <a href=\"https://en.wikipedia.org/wiki/Quasi-Newton_method\" target=\"_blank\">quasi-Newton methods</a>, which are used to either find zeroes or local maxima and minima of functions iteratively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a17cd-8800-47d6-9782-2ffde6356e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:epl_analytics] *",
   "language": "python",
   "name": "conda-env-epl_analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
