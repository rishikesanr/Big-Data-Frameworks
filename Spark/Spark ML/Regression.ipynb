{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50acc83-49c6-428b-a678-9608deb6d8d9",
   "metadata": {},
   "source": [
    "# Regression (Notebook for Databricks)\n",
    "\n",
    "We'll do the following steps to build our model, \n",
    "\n",
    "Steps:\n",
    "1. Use the features: **`bedrooms`**, **`bathrooms`**, **`bathrooms_na`**, **`minimum_nights`**, and **`number_of_reviews`** as input to your VectorAssembler.\n",
    "2. Build a Linear Regression Model\n",
    "3. Evaluate the **`RMSE`** and the **`R2`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef99d85-ef0d-4001-870d-5cd25676129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# from pyspark import SparkConf, SparkContext\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil import relativedelta\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_timestamp, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import collect_list, collect_set, concat, first, array_distinct, col, size, expr\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d714c09-0c48-452b-bec7-39d96a35f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the spark session (Although it is not required if notebook directly ran in Databricks)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Data Analysis in Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c085e-559f-41a6-a452-208675cce099",
   "metadata": {},
   "source": [
    "## Load Dataset and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1fccde4-f0ee-49c7-8dfb-c2a0b6864394",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./cleaned_listings.csv\"\n",
    "# airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "\n",
    "#Read the cleaned csv file \n",
    "airbnb_df = spark.read.csv(file_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75927add-213d-43a2-af5d-d21b92d7045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 18:50:21 WARN Instrumentation: [a14b8fd5] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/05/01 18:50:22 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/01 18:50:22 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "#Vectorize the dependent variables \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"bedrooms\",\"bathrooms\",\"bathrooms_na\",\"minimum_nights\",\"number_of_reviews\"],outputCol=\"features\")\n",
    "\n",
    "vtrain_df = vec_assembler.transform(train_df)\n",
    "vtest_df = vec_assembler.transform(test_df)\n",
    "\n",
    "lr_model = LinearRegression(labelCol=\"price\").fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b34aab-37a1-48ba-a1a9-bcca6f6484d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 419.0121578376188\n",
      "R2 is 0.07456171275982248\n"
     ]
    }
   ],
   "source": [
    "pred_df = lr_model.transform(vtest_df)\n",
    "\n",
    "regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "rmse = regression_evaluator.evaluate(pred_df)\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b500f04-79a0-4909-a967-82cf09bf6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOS R-squared is low. Note this notebook is only for practice for Spark ML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2448ff98-c151-4fa8-bfa4-cbb6d961166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedrooms 114.17777449189113\n",
      "bathrooms -5.8636569331674835\n",
      "bathrooms_na -93.46199646696445\n",
      "minimum_nights 0.11479885115899408\n",
      "number_of_reviews -0.2841304691298576\n",
      "intercept: 89.84420157032639\n"
     ]
    }
   ],
   "source": [
    "for col, coef in zip(vec_assembler.getInputCols(), lr_model.coefficients):\n",
    "    print(col, coef)\n",
    "  \n",
    "print(f\"intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f228e-c207-4b10-b5f1-3e37d8507d38",
   "metadata": {},
   "source": [
    "## Additional notes on Spark Distributed Computing\n",
    "\n",
    "## Distributed Setting\n",
    "\n",
    "Although we can quickly solve for the parameters when the data is small, the closed form solution doesn't scale well to large datasets. \n",
    "\n",
    "Spark uses the following approach to solve a linear regression problem:\n",
    "\n",
    "* First, Spark tries to use matrix decomposition to solve the linear regression problem. \n",
    "* If it fails, Spark then uses <a href=\"https://spark.apache.org/docs/latest/ml-advanced.html#limited-memory-bfgs-l-bfgs\" target=\"_blank\">L-BFGS</a> to solve for the parameters. L-BFGS is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables. The <a href=\"https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\" target=\"_blank\">BFGS</a> method belongs to <a href=\"https://en.wikipedia.org/wiki/Quasi-Newton_method\" target=\"_blank\">quasi-Newton methods</a>, which are used to either find zeroes or local maxima and minima of functions iteratively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13001f80-a7d8-4a4a-9447-a1b677b6280f",
   "metadata": {},
   "source": [
    "## Improving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16f2dbd3-1c2a-4eee-9260-73120299424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc214e-545e-4033-bae2-37fca303d227",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b833c13b-e5f7-4d7a-8b3f-ed7b0e687b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Variables \n",
    "\n",
    "#One Hot Encoding\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "ohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "ohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8175606-8f52-4b57-aa38-7a43b2c1be33",
   "metadata": {},
   "source": [
    "#### Vector Assembler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ca0a694-4ca7-4a98-bd45-d290730c9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assembler_inputs = ohe_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb48ed-68bc-4d7d-add7-80dd42bf9c2f",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1fe490c-e944-4989-af77-bf0cc62d5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36244133-ef57-475a-924d-0101c0629137",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307197f2-6d25-4690-926c-c02d6dd21ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = [string_indexer, ohe_encoder, vec_assembler, lr]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a12f0-e65e-4218-bdd4-9663cf93b3ee",
   "metadata": {},
   "source": [
    "#### Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49d1658b-541f-4709-8aea-99278e7d991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.write().overwrite().save('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85b973f-2712-40e0-b494-95765c11ad0e",
   "metadata": {},
   "source": [
    "#### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9faa80b9-4d6a-42d0-b4cc-219e3c212abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "saved_pipeline_model = PipelineModel.load('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d52b08-cf16-4d13-a591-f3edfb1c54f0",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbfde2ad-a1c8-4795-8167-2b82e901bec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, price: double, prediction: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_df = saved_pipeline_model.transform(test_df)\n",
    "\n",
    "display(pred_df.select(\"features\", \"price\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "194a7419-bff6-4819-813b-9c8679f6fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e7f7e-22b5-49d0-ab96-1740ce149f54",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c22df66-e481-4d94-ac63-1b98184d91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "\n",
    "# rmse = regression_evaluator.evaluate(pred_df)\n",
    "# r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "# print(f\"RMSE is {rmse}\")\n",
    "# print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5820b2-0f32-4cdb-8330-45442f8ad00a",
   "metadata": {},
   "source": [
    "## Using RFormula \n",
    "\n",
    "Instead of manually specifying which columns are categorical to the StringIndexer and OneHotEncoder, RFormula will do it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc4b046-321f-40f6-9769-3435af58689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "r_formula = RFormula(formula=\"price~.\",\n",
    "                     featuresCol=\"features\",\n",
    "                     handleInvalid=\"skip\",\n",
    "                     labelCol=\"price\")\n",
    "lr = LinearRegression(labelCol=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b085af8f-2711-47d7-9429-b4d44a057771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"featuresCol: features column name. (default: features, current: features)\\nforceIndexLabel: Force to index label whether it is numeric or string (default: False)\\nformula: R model formula (current: price~.)\\nhandleInvalid: how to handle invalid entries. Options are 'skip' (filter out rows with invalid values), 'error' (throw an error), or 'keep' (put invalid data in a special additional bucket, at index numLabels). (default: error, current: skip)\\nlabelCol: label column name. (default: label, current: price)\\nstringIndexerOrderType: How to order categories of a string feature column used by StringIndexer. The last category after ordering is dropped when encoding strings. Supported options: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc. The default value is frequencyDesc. When the ordering is set to alphabetDesc, RFormula drops the same category as R when encoding strings. (default: frequencyDesc)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_formula.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73a471d-058f-4841-a761-5ed571d97a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/02 10:10:21 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/05/02 10:10:26 WARN Instrumentation: [9d20d039] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/05/02 10:10:27 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/02 10:10:27 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/05/02 10:10:27 WARN Instrumentation: [9d20d039] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 405.6176153007995\n",
      "R2 is 0.13422393436448787\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[r_formula,lr])\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "pred_df = pipeline_model.transform(test_df)\n",
    "\n",
    "regression_evaluator = RegressionEvaluator(labelCol=\"price\")\n",
    "\n",
    "rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62fcde-c2b1-4e02-af2b-03d572ad53f3",
   "metadata": {},
   "source": [
    "### Use Log Transformmation for Price\n",
    "\n",
    "Since we came to the price is super right skewed, a log transformed price will have a much better distribution \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a5c25e-5aba-418d-8de2-63ec4b61e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/02 10:20:17 WARN Instrumentation: [2d12f4b4] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/05/02 10:20:17 WARN Instrumentation: [2d12f4b4] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, log\n",
    "\n",
    "log_train_df = train_df.withColumn(\"logprice\",log(\"price\"))\n",
    "log_test_df = test_df.withColumn(\"logprice\",log(\"price\"))\n",
    "\n",
    "r_formula = RFormula(formula=\"logprice ~ . - price\",handleInvalid=\"skip\") # Look at handleInvalid\n",
    "lr.setLabelCol(\"logprice\")\n",
    "pipeline = Pipeline(stages=[r_formula, lr])\n",
    "pipeline_model = pipeline.fit(log_train_df)\n",
    "pred_df = pipeline_model.transform(log_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e502f945-0056-4895-91a8-f63555bf293d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 408.09168053567373\n",
      "R2 is 0.12363011935789525\n"
     ]
    }
   ],
   "source": [
    "# In order to interpret our RMSE, we need to convert our predictions back from logarithmic scale.\n",
    "from pyspark.sql.functions import exp\n",
    "\n",
    "exp_df = pred_df.withColumn(\"prediction\",exp(\"prediction\"))\n",
    "\n",
    "rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(exp_df)\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(exp_df)\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871b846-75bb-4ab0-b70b-41f090e80f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3df267-2260-479e-bc1a-ef37eccf4f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:epl_analytics] *",
   "language": "python",
   "name": "conda-env-epl_analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
